{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb94c659",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9863f28",
   "metadata": {},
   "source": [
    "Data ingestion -> Document Store (Azure AI Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e88d6b3",
   "metadata": {},
   "source": [
    "## 1. Ingest pdf(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cd6f42",
   "metadata": {},
   "source": [
    "Ingest pdf(s) in `/data` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d3cd28",
   "metadata": {},
   "source": [
    "## 2. Run OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50950fa8",
   "metadata": {},
   "source": [
    "Run OCR to extract text from each page. Mistral document model (https://docs.mistral.ai/capabilities/document_ai), it is on Azure AI foundry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beed7fd",
   "metadata": {},
   "source": [
    "## 2.1 (optional) Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50d5f14",
   "metadata": {},
   "source": [
    "In case the OCR text is very messy, clean it here before chunking, or results will be useless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2456e6",
   "metadata": {},
   "source": [
    "This is probably not needed because Mistral is apparently a good OCR model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727842ce",
   "metadata": {},
   "source": [
    "## 3. Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68974c8c",
   "metadata": {},
   "source": [
    "Chunk OCR test with a simple simple textsplitter (https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-chunk-documents#langchain-data-chunking-example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff2233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample code\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# split documents into text and embeddings\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "   chunk_size=1000, \n",
    "   chunk_overlap=200,\n",
    "   length_function=len,\n",
    "   is_separator_regex=False\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "\n",
    "print(chunks[20])\n",
    "print(chunks[21])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed0e7dd",
   "metadata": {},
   "source": [
    "## 4. Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116a187b",
   "metadata": {},
   "source": [
    "Generate vector embeddings per chunk using the Azure OpenAI embedding model. (https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/embeddings?view=foundry-classic&tabs=csharp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e7cff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings_model = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-3-large\",\n",
    "    openai_api_version=\"2023-05-15\",\n",
    ")\n",
    "\n",
    "# embeddings are generated in next step directly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6ae95e",
   "metadata": {},
   "source": [
    "## 5. Vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789ef5cd",
   "metadata": {},
   "source": [
    "Index in Azure AI Search: store chunk text + metadata (document id, page number, folder, category, source_link) + embedding vector; enable vector search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a860a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_to_upload = []\n",
    "\n",
    "print(\"Generating embeddings and preparing payload...\")\n",
    "for chunk in final_chunks:\n",
    "    # Extract metadata that PyPDFLoader provided\n",
    "    # Note: PyPDFLoader is 0-indexed, so add 1 for humans\n",
    "    page_num = chunk.metadata.get('page', 0) + 1 \n",
    "    \n",
    "    # Create your formatted location string\n",
    "    location_string = f\"Source: {filename}, Page: {page_num}\"\n",
    "\n",
    "    # Generate Embedding (LangChain handles the API call)\n",
    "    vector = embeddings_model.embed_query(chunk.page_content)\n",
    "\n",
    "    # Map to your Azure Search Index Schema\n",
    "    azure_doc = {\n",
    "        \"id\": str(uuid.uuid4()),         # Generate unique key\n",
    "        \"content\": chunk.page_content,   # The text text\n",
    "        \"contentVector\": vector,         # The 1536-dim embedding\n",
    "        \"location\": location_string      # get location\n",
    "    }\n",
    "    documents_to_upload.append(azure_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c1cd30",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cbfc8a",
   "metadata": {},
   "source": [
    "Validate end-to-end: run a few test queries, confirm top results point back to the right page/chunk, and iterate on chunking/cleaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd9778f",
   "metadata": {},
   "source": [
    "## 7. Use AI Search MCP on GHCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55899af",
   "metadata": {},
   "source": [
    "This will be needed to set up an MCP server for AI search (for vector/hybrid search): https://github.com/tomgutt/azure-ai-search-mcp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11df617",
   "metadata": {},
   "source": [
    "## 8. Integrate MCP with OpenWebUI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee545dc7",
   "metadata": {},
   "source": [
    "OpenWebUI doesn't support stdio MCP configurations natively, use `mcpo` python library for it to work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
